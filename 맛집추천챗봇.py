import json
import math
import re

import numpy as np
import pandas as pd
import streamlit as st
from openai import OpenAI
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

from dotenv import load_dotenv
import os

# ì‹ë‹¹ ì •ë³´ url ì„ ìœ„í•´ì„œ
# url ì— í•œê¸€ ì‹ë‹¹ ì´ë¦„ì´ ë“¤ì–´ê°€ë©´ ì—ëŸ¬ê°€ ë‚  ìˆ˜ ìˆì–´ì„œ ì¸ì½”ë”©ì„ ìœ„í•œ ëª¨ë“ˆì„ ê°€ì ¸ì˜´
from urllib.parse import quote

load_dotenv()

# =========================
# ì„¤ì •
# =========================
st.set_page_config(page_title="ì‚¬ìš©ì ì¡°ê±´ ê¸°ë°˜ ë§›ì§‘ ì¶”ì²œ", page_icon="ğŸ½ï¸", layout="wide")

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    st.error("OPENAI_API_KEYê°€ í™˜ê²½ë³€ìˆ˜(.env)ì—ì„œ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

client = OpenAI(api_key=OPENAI_API_KEY)

# âœ… ì´ë¯¸ ë§Œë“¤ì–´ì§„ ë¦¬ë·°/ê°ì„± ì„ë² ë”© Chroma DB
DB_PATH = r"./vector_db"
COLLECTION_NAME = "hongdae_restaurants"

embedding_model = OpenAIEmbeddings(
    model="text-embedding-3-large",
    openai_api_key=OPENAI_API_KEY
)

db = Chroma(
    collection_name=COLLECTION_NAME,
    embedding_function=embedding_model,
    persist_directory=DB_PATH
)

# ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
CHROMA_K = 80     # Chromaì—ì„œ ê°€ì ¸ì˜¤ëŠ” ë¬¸ì„œ ìˆ˜(ë¦¬ë·° ë¬¸ì„œ ì¤‘ë³µ ëŒ€ë¹„ ë„‰ë„‰íˆ)
FINAL_K = 5       # ìµœì¢… í›„ë³´(= nextìš©)

# âœ… ê±°ë¦¬(ë„ë³´ì‹œê°„) ê°€ì¤‘ì¹˜(íŒ¨ë„í‹°)
LAMBDA_DISTANCE = 0.15  # 0~1 ê¶Œì¥ (0ì´ë©´ ê±°ë¦¬ ì˜í–¥ ì—†ìŒ)

# =========================
# CSV ë¡œë“œ (ë©”íƒ€ë°ì´í„° ì „ìš©)
# =========================
@st.cache_data
def load_meta_csv():
    df = pd.read_csv("/Users/ijunseong/Downloads/ì‹ë‹¹DB_í†µí•©_ë„ë³´ì¶”ê°€_ìµœìµœì¢…ìˆ˜ì •.csv")

    must_text = ["ì‚¬ì—…ì¥ëª…", "ì—…íƒœêµ¬ë¶„ëª…", "ëŒ€í‘œë©”ë‰´_ë©”ë‰´", "ëŒ€í‘œë©”ë‰´_ê°€ê²©", "ì§€ë²ˆì£¼ì†Œ"]
    for c in must_text:
        if c not in df.columns:
            df[c] = ""
        df[c] = df[c].fillna("").astype(str)

    must_num = ["ë„ë³´ê±°ë¦¬_km", "ë„ë³´ì‹œê°„_ë¶„"]
    for c in must_num:
        if c not in df.columns:
            df[c] = np.nan
        df[c] = pd.to_numeric(df[c], errors="coerce")

    df["ì‚¬ì—…ì¥ëª…"] = df["ì‚¬ì—…ì¥ëª…"].astype(str).str.strip()
    return df

df_meta = load_meta_csv()

@st.cache_data
def build_name_index(df):
    temp = df.copy()
    temp = temp[temp["ì‚¬ì—…ì¥ëª…"] != ""]
    temp = temp.drop_duplicates(subset=["ì‚¬ì—…ì¥ëª…"], keep="first")
    return temp.set_index("ì‚¬ì—…ì¥ëª…")

meta_index = build_name_index(df_meta)

# =========================
# ìœ í‹¸
# =========================
def clean_llm_text(text: str) -> str:
    lines = []
    for line in text.splitlines():
        if line.strip().startswith("ìš”ì•½:"):
            continue
        lines.append(line)
    return "\n".join(lines).strip()

def safe_num(x):
    try:
        if x is None:
            return None
        if isinstance(x, float) and math.isnan(x):
            return None
        return float(x)
    except Exception:
        return None

# âœ… ë©”ë‰´/ê°€ê²© ë¬¸ìì—´ì„ "ë©”ë‰´ : ê°€ê²©" í˜•íƒœë¡œ ê¹”ë”í•˜ê²Œ ë§Œë“¤ê¸°
def _split_items(s: str):
    """'A | B | C' ë˜ëŠ” 'A,B,C' ê°™ì€ ë¬¸ìì—´ì„ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬"""
    if s is None:
        return []
    s = str(s).strip()
    if not s:
        return []

    # | ìš°ì„ , ì—†ìœ¼ë©´ ì‰¼í‘œë¡œ ë¶„ë¦¬
    if "|" in s:
        parts = [p.strip() for p in s.split("|")]
    else:
        parts = [p.strip() for p in s.split(",")]

    return [p for p in parts if p]

def format_menu_price_lines(menu: str, price: str, max_items: int = 8):
    menus = _split_items(menu)
    prices = _split_items(price)

    lines = []
    n = min(len(menus), len(prices))

    if n > 0:
        for i in range(n):
            lines.append(f"- {menus[i]} : {prices[i]}")
    else:
        # ì§ì´ ì•ˆ ë§ê±°ë‚˜ í•œìª½ë§Œ ìˆìœ¼ë©´ ìˆëŠ” ê²ƒë§Œ í‘œì‹œ
        if menus and not prices:
            for m in menus[:max_items]:
                lines.append(f"- {m}")
        elif prices and not menus:
            for p in prices[:max_items]:
                lines.append(f"- {p}")
        elif menu or price:
            lines.append(f"- {menu} : {price}".strip(" :"))

    if max_items is not None:
        lines = lines[:max_items]
    return lines

# =========================
# 1) ì‚¬ìš©ì ë¬¸ì¥ -> CSV í•˜ë“œí•„í„° ì¡°ê±´ ì¶”ì¶œ
# =========================
def parse_filter_condition(q: str):
    cond = {}

    m = re.search(r"(\d+)\s*ë¶„", q)
    if m:
        cond["max_time"] = int(m.group(1))

    m = re.search(r"(\d+(\.\d+)?)\s*km", q.lower())
    if m:
        cond["max_dist"] = float(m.group(1))

    # ì—…íƒœ: CSV ì—…íƒœêµ¬ë¶„ëª… unique ì¤‘ ë¬¸ì¥ì— í¬í•¨ë˜ëŠ” ê±¸ ì°¾ìŒ
    for keyword in df_meta["ì—…íƒœêµ¬ë¶„ëª…"].dropna().unique():
        kw = str(keyword).strip()
        if kw and kw in q:
            cond["ì—…íƒœ"] = kw
            break

    return cond

# =========================
# 2) CSVì—ì„œ 1ì°¨ í•˜ë“œí•„í„°ë¡œ allowed_names ë§Œë“¤ê¸°
# =========================
def hard_filter_names_from_csv(user_text: str, max_minutes=None):
    cond = parse_filter_condition(user_text)
    if max_minutes is not None:
        cond["max_time"] = int(max_minutes)

    temp = df_meta.copy()

    if "max_time" in cond:
        temp = temp[temp["ë„ë³´ì‹œê°„_ë¶„"].notna() & (temp["ë„ë³´ì‹œê°„_ë¶„"] <= cond["max_time"])]

    if "max_dist" in cond:
        temp = temp[temp["ë„ë³´ê±°ë¦¬_km"].notna() & (temp["ë„ë³´ê±°ë¦¬_km"] <= cond["max_dist"])]

    if "ì—…íƒœ" in cond:
        temp = temp[temp["ì—…íƒœêµ¬ë¶„ëª…"].astype(str).str.contains(cond["ì—…íƒœ"], na=False)]

    names = set(temp["ì‚¬ì—…ì¥ëª…"].astype(str).str.strip().tolist())
    names.discard("")
    return names, cond

# =========================
# 3) Chroma ê²€ìƒ‰ì„ allowed_names ì§‘í•© ë‚´ë¶€ë¡œ ì œí•œ($in)
#    + âœ… ê±°ë¦¬(ë„ë³´ì‹œê°„) ê°€ì¤‘ì¹˜ë¡œ ì¬ë­í‚¹
# =========================
def chroma_search_only_allowed(
    query_text: str,
    allowed_names: set,
    exclude_names: set,
    top_k=FINAL_K
):
    where = {"ì‚¬ì—…ì¥ëª…": {"$in": list(allowed_names)}}

    results = db.similarity_search_with_relevance_scores(
        query_text,
        k=CHROMA_K,
        filter=where
    )

    rows = []
    for doc, rel in results:
        meta = doc.metadata or {}
        name = str(meta.get("ì‚¬ì—…ì¥ëª…", "")).strip()
        if not name:
            continue
        if name in exclude_names:
            continue

        if name in meta_index.index:
            walk = safe_num(meta_index.loc[name].get("ë„ë³´ì‹œê°„_ë¶„", None))
        else:
            walk = safe_num(meta.get("ë„ë³´ì‹œê°„_ë¶„", None))

        rows.append((doc, float(rel), walk))

    if not rows:
        return []

    times = np.array([r[2] if r[2] is not None else np.nan for r in rows], dtype=float)
    valid = np.isfinite(times)

    if valid.any():
        tmin, tmax = float(np.nanmin(times)), float(np.nanmax(times))
        if tmax == tmin:
            tnorm = np.zeros_like(times)
        else:
            tnorm = (times - tmin) / (tmax - tmin)
        tnorm[~valid] = 1.0
    else:
        tnorm = np.zeros_like(times)

    scored = []
    for (doc, rel, _walk), pen in zip(rows, tnorm):
        final_score = rel - (LAMBDA_DISTANCE * float(pen))
        scored.append((final_score, doc))

    scored.sort(key=lambda x: x[0], reverse=True)

    picked = []
    seen = set()
    for _score, doc in scored:
        name = str((doc.metadata or {}).get("ì‚¬ì—…ì¥ëª…", "")).strip()
        if not name or name in seen:
            continue
        seen.add(name)
        picked.append(doc)
        if len(picked) >= top_k:
            break

    return picked

# =========================
# GPTë¡œ ì˜ë„/ì¡°ê±´ ì¶”ì¶œ
# =========================
def parse_user_message(history_messages, user_message):
    recent = history_messages[-8:]
    recent_text = ""
    for m in recent:
        role = "ì‚¬ìš©ì" if m["role"] == "user" else "ì±—ë´‡"
        recent_text += f"{role}: {m['content']}\n"

    system = """
ë„ˆëŠ” "ë§›ì§‘ ì¶”ì²œ ì±—ë´‡"ì˜ ì˜ë„ íŒŒì„œë‹¤. ì‚¬ìš©ìì˜ ìì—°ì–´ ë°œí™”ë¥¼ ì•„ë˜ JSONìœ¼ë¡œ êµ¬ì¡°í™”í•œë‹¤.
ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥í•œë‹¤. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ê¸ˆì§€.

JSON ìŠ¤í‚¤ë§ˆ:
{
  "search_query": string|null,
  "max_minutes": number|null,
  "want_next": boolean,
  "reset": boolean,
  "hard_constraints_text": string|null
}

ê·œì¹™:
- want_next=true: ì‚¬ìš©ìê°€ 'ë‹¤ì‹œ ì¶”ì²œ', 'ë‹¤ë¥¸ ë°', 'ë³„ë¡œì•¼', 'ë§ˆìŒì— ì•ˆ ë“¤ì–´', 'ë‹¤ìŒ', 'ë‹¤ë¥¸ ê³³' ë“±ìœ¼ë¡œ
  "ë°”ë¡œ ë‹¤ìŒ í›„ë³´"ë¥¼ ì›í•˜ë©´ true.
  ë‹¨, ì‚¬ìš©ìê°€ ë©”ë‰´/ë¶„ìœ„ê¸°/ì¡°ê±´ì„ í¬ê²Œ ë°”ê¾¸ë©´ want_next=falseë¡œ ë‘ê³  search_queryë¥¼ ìƒˆë¡œ êµ¬ì„±.
- max_minutes: '15ë¶„', '20ë¶„ ì´ë‚´', '10ë¶„ ì•ˆìª½' ë“± ë„ë³´ì‹œê°„ ì œí•œì´ ìˆìœ¼ë©´ ìˆ«ìë§Œ.
- reset=true: 'ì²˜ìŒë¶€í„°', 'ë¦¬ì…‹', 'ì¡°ê±´ ì´ˆê¸°í™”', 'ìƒˆë¡œ ì°¾ì' ë“±.
- search_query: ì§€ê¸ˆ í„´ì˜ ì¶”ì²œ ì˜ë„(ë©”ë‰´/ë¶„ìœ„ê¸°/ìƒí™© í¬í•¨) í•œ ë¬¸ì¥ ìš”ì•½.
- hard_constraints_text: ê°•ì¡° ì¡°ê±´(ê°€ì„±ë¹„/ì¡°ìš©í•¨/ì¸í…Œë¦¬ì–´/ë°ì´íŠ¸ ë“±) ì§§ê²Œ ìš”ì•½.
"""

    user = f"""
ìµœê·¼ ëŒ€í™”:
{recent_text}

ì´ë²ˆ ì‚¬ìš©ì ë°œí™”:
{user_message}
"""

    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "system", "content": system},
                  {"role": "user", "content": user}],
        temperature=0.0
    )

    raw = resp.choices[0].message.content.strip()
    try:
        data = json.loads(raw)
    except Exception:
        data = {"search_query": None, "max_minutes": None, "want_next": False, "reset": False, "hard_constraints_text": None}

    data.setdefault("search_query", None)
    data.setdefault("max_minutes", None)
    data.setdefault("want_next", False)
    data.setdefault("reset", False)
    data.setdefault("hard_constraints_text", None)
    return data

# =========================
# ì¶”ì²œ ë¬¸ì¥ ìƒì„± (ë©”íƒ€ë°ì´í„°ëŠ” CSV ê¸°ì¤€ìœ¼ë¡œ ì¶œë ¥)
# =========================
def generate_reco_text(user_message, state, doc):
    meta = doc.metadata or {}
    name = str(meta.get("ì‚¬ì—…ì¥ëª…", "")).strip()

    if name in meta_index.index:
        row = meta_index.loc[name]
        category = str(row.get("ì—…íƒœêµ¬ë¶„ëª…", "")).strip()
        addr = str(row.get("ì§€ë²ˆì£¼ì†Œ", "")).strip()
        walk_min = safe_num(row.get("ë„ë³´ì‹œê°„_ë¶„", None))
        menu = str(row.get("ëŒ€í‘œë©”ë‰´_ë©”ë‰´", "")).strip()
        price = str(row.get("ëŒ€í‘œë©”ë‰´_ê°€ê²©", "")).strip()
    else:
        category = str(meta.get("ì—…íƒœêµ¬ë¶„ëª…", "")).strip()
        addr = str(meta.get("ì§€ë²ˆì£¼ì†Œ", "")).strip()
        walk_min = safe_num(meta.get("ë„ë³´ì‹œê°„_ë¶„", None))
        menu = str(meta.get("ëŒ€í‘œë©”ë‰´_ë©”ë‰´", "")).strip()
        price = str(meta.get("ëŒ€í‘œë©”ë‰´_ê°€ê²©", "")).strip()

    walk_line = f"ğŸš¶ ë„ë³´ ì•½ {walk_min:g}ë¶„" if walk_min is not None else "ğŸš¶ ë„ë³´ì‹œê°„ ì •ë³´ ì—†ìŒ"

    # ë„¤ì´ë²„ ì§€ë„ ê²€ìƒ‰ url ìƒì„±
    search_keyword = name
    if addr:
        splits = addr.split()
        dong_name = next((s for s in splits if s.endswith("ë™")), "")
        if dong_name:
            search_keyword = f"{dong_name} {name}"

    query_encoded = quote(search_keyword)
    map_url = f"https://map.naver.com/p/search/{query_encoded}"

    system = "ë„ˆëŠ” í™ëŒ€ ë§›ì§‘ ì¶”ì²œ ì „ë¬¸ ì±—ë´‡ì´ë‹¤."
    prompt = f"""
ì‚¬ìš©ì ë°œí™”: {user_message}
í˜„ì¬ ì¡°ê±´(state): {json.dumps(state, ensure_ascii=False)}

ì¶”ì²œí•  ì‹ë‹¹ ì •ë³´:
- ì‹ë‹¹ëª…: {name}
- ì—…íƒœ: {category}
- ì£¼ì†Œ: {addr}

ìš”êµ¬ì‚¬í•­:
- ì‹ë‹¹ì€ ì´ 1ê³³ë§Œ ì¶”ì²œí•œë‹¤.
- 3~5ë¬¸ì¥ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê³  í˜„ì‹¤ì ìœ¼ë¡œ ì¶”ì²œ ì´ìœ ë¥¼ ë§í•œë‹¤.
- "ìš”ì•½:" ê°™ì€ ìš”ì•½ ë¼ì¸ì€ ì ˆëŒ€ ì“°ì§€ ë§ˆë¼.
- ê´‘ê³  ë¬¸êµ¬ì²˜ëŸ¼ ê³¼ì¥í•˜ì§€ ë§ˆë¼.
"""

    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "system", "content": system},
                  {"role": "user", "content": prompt}],
        temperature=0.4
    )

    body = clean_llm_text(resp.choices[0].message.content.strip())

    out = []
    out.append(f"### ğŸ½ï¸ {name}")
    out.append(walk_line)
    out.append("")
    out.append(body)

    # âœ… ì—¬ê¸°ë§Œ ë°”ë€œ: ë©”ë‰´/ê°€ê²©ì„ "ë©”ë‰´ : ê°€ê²©" ë¦¬ìŠ¤íŠ¸ë¡œ ì¶œë ¥
    if menu or price:
        out.append("")
        out.append("**ëŒ€í‘œë©”ë‰´ / ê°€ê²©**")
        out.extend(format_menu_price_lines(menu, price, max_items=8))

    if addr:
        out.append(f"\nğŸ“[ì‹ë‹¹ ìì„¸íˆ ë³´ê¸°]({map_url})")

    return "\n".join(out).strip(), name

# =========================
# UI
# =========================
st.title("ì‚¬ìš©ì ì¡°ê±´ ê¸°ë°˜ ë§›ì§‘ ì¶”ì²œ")
st.caption("ì¡°ê±´ì„ ë§í•˜ë©´ 1ê³³ë§Œ ì¶”ì²œí•´ì¤˜ìš”. ë§ˆìŒì— ì•ˆ ë“¤ë©´ â€˜ë‹¤ë¥¸ ê³³ ì¶”ì²œí•´ì¤˜â€™ë¼ê³  ë§í•˜ë©´ ë‹¤ìŒ í›„ë³´ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")

if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "state" not in st.session_state:
    st.session_state["state"] = {"search_query": None, "max_minutes": None, "hard_constraints_text": None}

if "result_docs" not in st.session_state:
    st.session_state["result_docs"] = []
if "result_idx" not in st.session_state:
    st.session_state["result_idx"] = 0

if "shown_names" not in st.session_state:
    st.session_state["shown_names"] = set()
if "reco_history" not in st.session_state:
    st.session_state["reco_history"] = []

with st.sidebar:
    st.subheader("ğŸ“Œ ì¶”ì²œ ë‚´ì—­")
    if st.session_state["reco_history"]:
        for i, nm in enumerate(reversed(st.session_state["reco_history"]), start=1):
            st.write(f"{i}. {nm}")
    else:
        st.write("ì•„ì§ ì¶”ì²œí•œ ì‹ë‹¹ì´ ì—†ìŠµë‹ˆë‹¤.")

    if st.button("ëŒ€í™”/ìƒíƒœ ì´ˆê¸°í™”"):
        st.session_state.clear()
        st.rerun()

for m in st.session_state["messages"]:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

user_input = st.chat_input("(ì˜ˆ: 20ë¶„ ì•ˆì— ê°ˆ ìˆ˜ ìˆëŠ” ê°€ì„±ë¹„ ì¢‹ì€ íŒŒìŠ¤íƒ€ ì§‘ ì¶”ì²œí•´ì¤˜)")

if user_input:
    st.session_state["messages"].append({"role": "user", "content": user_input})

    with st.chat_message("user"):
        st.markdown(user_input)

    parsed = parse_user_message(st.session_state["messages"], user_input)

    if parsed.get("reset"):
        st.session_state["state"] = {
            "search_query": parsed.get("search_query"),
            "max_minutes": parsed.get("max_minutes"),
            "hard_constraints_text": parsed.get("hard_constraints_text"),
        }
        st.session_state["result_docs"] = []
        st.session_state["result_idx"] = 0
        st.session_state["shown_names"] = set()
        st.session_state["reco_history"] = []
    else:
        if parsed.get("search_query"):
            st.session_state["state"]["search_query"] = parsed.get("search_query")
        if parsed.get("hard_constraints_text"):
            st.session_state["state"]["hard_constraints_text"] = parsed.get("hard_constraints_text")
        if parsed.get("max_minutes") is not None:
            st.session_state["state"]["max_minutes"] = parsed.get("max_minutes")

    want_next = bool(parsed.get("want_next"))

    with st.chat_message("assistant"):
        with st.spinner("ì¡°ê±´ì— ë§ëŠ” ì‹ë‹¹ì„ ì°¾ëŠ” ì¤‘..."):
            state = st.session_state["state"]
            search_query = state.get("search_query")
            max_minutes = state.get("max_minutes")
            hard_text = state.get("hard_constraints_text")

            if not search_query:
                msg = "ì›í•˜ëŠ” ë©”ë‰´/ë¶„ìœ„ê¸°ë¥¼ ì¡°ê¸ˆë§Œ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§í•´ì¤„ë˜ìš”?"
                st.markdown(msg)
                st.session_state["messages"].append({"role": "assistant", "content": msg})
            else:
                effective_query = search_query if not hard_text else f"{search_query}. ì¡°ê±´: {hard_text}"

                if want_next and st.session_state["result_docs"] and st.session_state["result_idx"] < len(st.session_state["result_docs"]):
                    doc = st.session_state["result_docs"][st.session_state["result_idx"]]
                    st.session_state["result_idx"] += 1
                else:
                    allowed_names, cond = hard_filter_names_from_csv(user_input, max_minutes=max_minutes)

                    if not allowed_names:
                        msg = "CSV ì¡°ê±´(ì‹œê°„/ê±°ë¦¬/ì—…íƒœ)ì— ë§ëŠ” ì‹ë‹¹ì´ ì—†ì–´ìš” ğŸ˜¢ ì¡°ê±´ì„ ì™„í™”í•´ì£¼ì‹œë©´ ë‹¤ì‹œ ì°¾ì•„ë³¼ê²Œìš”."
                        st.markdown(msg)
                        st.session_state["messages"].append({"role": "assistant", "content": msg})
                        doc = None
                    else:
                        try:
                            docs = chroma_search_only_allowed(
                                query_text=effective_query,
                                allowed_names=allowed_names,
                                exclude_names=st.session_state["shown_names"],
                                top_k=FINAL_K
                            )
                        except Exception as e:
                            st.error(f"Chroma $in í•„í„°ê°€ ì§€ì›ë˜ì§€ ì•Šê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                            docs = []

                        st.session_state["result_docs"] = docs
                        st.session_state["result_idx"] = 0

                        if not docs:
                            msg = "ì¡°ê±´ì— ë§ëŠ” ì‹ë‹¹ì€ ìˆëŠ”ë°, ê°ì„±/ë¦¬ë·° ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¹­ì´ ì•½í—¸ì–´ìš” ğŸ˜¢ í‘œí˜„ì„ ë°”ê¿”ì„œ ë§í•´ì¤„ë˜ìš”?"
                            st.markdown(msg)
                            st.session_state["messages"].append({"role": "assistant", "content": msg})
                            doc = None
                        else:
                            doc = docs[st.session_state["result_idx"]]
                            st.session_state["result_idx"] += 1

                if doc is not None:
                    name = (doc.metadata or {}).get("ì‚¬ì—…ì¥ëª…")
                    if name:
                        st.session_state["shown_names"].add(name)

                    reco_text, reco_name = generate_reco_text(user_input, state, doc)
                    st.markdown(reco_text)
                    st.session_state["messages"].append({"role": "assistant", "content": reco_text})

                    if reco_name and (not st.session_state["reco_history"] or st.session_state["reco_history"][-1] != reco_name):
                        st.session_state["reco_history"].append(reco_name)

                    # ì´ì „ ë‹µë³€ ì”ìƒ ì œê±°
                    st.rerun()
